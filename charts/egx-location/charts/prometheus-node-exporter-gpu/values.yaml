# Default values for prometheus-node-exporter.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
replicaCount: 1
maxUnavailable: "10%"

image:
  pullSecrets:
    - "quayio-userpass"

# node exporter container
container:
  image:
    repository: quay.io/prometheus/node-exporter
    tag: v0.18.0
    pullPolicy: IfNotPresent

  args:
  - --path.rootfs=/host
  - --collector.textfile.directory=/run/prometheus

  service:
    type: ClusterIP
    port: 9100
    targetPort: 9100

  resources:
    requests:
      cpu: 200m
      memory: 512Mi
    limits:
      cpu: 200m
      memory: 512Mi

  volumeMounts:
  - name: rootfs
    mountPath: /host
    readOnly:  true
  - name: collector-textfiles
    readOnly: true
    mountPath: /run/prometheus

# dcgm exporter container
containerDCGM:
  image:
    repository: nvidia/dcgm-exporter
    tag: 1.4.6
    pullPolicy: Always

  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 500m
      memory: 512Mi

  volumeMounts:
  - name: device-metrics
    mountPath: /run/prometheus

# dcgm container with dcp
container3DCP:
  image:
    repository: quay.io/nvidia/dcgm-exporter-dcp
    tag: 1.6.3
    pullPolicy: Always

  volumeMounts:
  - name: device-metrics
    mountPath: /run/prometheus

# dcgm container with dcp v 1.6.6
container3DCP166:
  image:
    repository: quay.io/nvidia/dcgm-exporter-dcp
    tag: 1.6.6
    pullPolicy: Always

  volumeMounts:
  - name: device-metrics
    mountPath: /run/prometheus

# dcgm pod details exporter
dcgmPodExporterContainer:
  image:
    repository: nvidia/pod-devices-exporter
    tag: 1.4.6-dcgm
    pullPolicy: Always

  resources:
    requests:
      cpu: 200m
      memory: 512Mi
    limits:
      cpu: 200m
      memory: 512Mi

  volumeMounts:
  - name: pod-device-resources
    readOnly: true
    mountPath: /var/lib/kubelet/pod-resources
  - name: device-metrics
    readOnly: true
    mountPath: /run/prometheus
  - name: collector-textfiles
    mountPath: /run/dcgm

# fscache metrics exporter
fcacheExporterContainer:
  image:
    repository: quay.io/nvidia/fscache-exporter
    tag: 0.1
    pullPolicy: Always

  volumeMounts:
  - name: collector-textfiles
    mountPath: /run/prometheus

## Tolerations for use with node taints
## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
##
tolerations:
- effect: NoSchedule
  operator: Exists

volumes:
- name: collector-textfiles
  emptyDir:
    medium: Memory
- name: device-metrics
  emptyDir:
    medium: Memory
- name: pod-device-resources
  hostPath:
    path: /var/lib/kubelet/pod-resources
- name: rootfs
  hostPath:
    path: /rootfs
## If true, create & use RBAC resources resp. Pod Security Policies
##
global:
  rbacEnable: true
  pspEnable: true

# default rules are in templates/node.rules.yaml
# prometheusRules: {}

## Custom Labels to be added to ServiceMonitor
##
additionalServiceMonitorLabels: {}
##Custom Labels to be added to Prometheus Rules ConfigMap
##
additionalRulesConfigMapLabels: {}


service:
  type: ClusterIP
  port: 9100
  targetPort: 9100
  nodePort:
  annotations:
    prometheus.io/scrape: "true"

prometheus:
  monitor:
    enabled: false
    additionalLabels: {}
    namespace: ""

serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true
  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name:
  imagePullSecrets: []

rbac:
  ## If true, create & use RBAC resources
  ##
  create: true
  ## If true, create & use Pod Security Policy resources
  ## https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  pspEnabled: true

# for deployments that have node_exporter deployed outside of the cluster, list
# their addresses here
endpoints: []

# Expose the service to the host network
hostNetwork: true

## Assign a group of affinity scheduling rules
##
#affinity: {}
#   nodeAffinity:
#     requiredDuringSchedulingIgnoredDuringExecution:
#       nodeSelectorTerms:
#         - matchFields:
#             - key: metadata.name
#               operator: In
#               values:
#                 - target-host-name
## node affinity for where to deploy this daemonset
affinity: {}
#  nodeAffinity:
#    requiredDuringSchedulingIgnoredDuringExecution:
#      nodeSelectorTerms:
#      - matchExpressions:
#        - key: nodeType
#          operator: In
#          values:
#          - gpu

## Assign a nodeSelector if operating a hybrid cluster
##
nodeSelector: {}
#   beta.kubernetes.io/arch: amd64
#   beta.kubernetes.io/os: linux

tolerations:
  - effect: NoSchedule
    operator: Exists

## Assign a PriorityClassName to pods if set
# priorityClassName: ""

## Additional container arguments
##
extraArgs: {}
#   - --collector.diskstats.ignored-devices=^(ram|loop|fd|(h|s|v)d[a-z]|nvme\\d+n\\d+p)\\d+$

## Additional mounts from the host
##
extraHostVolumeMounts: {}
#  - name: <mountName>
#    hostPath: <hostPath>
#    mountPath: <mountPath>
#    readOnly: true|false
#    mountPropagation: None|HostToContainer|Bidirectional

# EGX settings:
locationName: ""
pushProxyClient:
  image:
    repository: quay.io/ligenvidia/pushprox-client
    tag: "1.0"
    pullPolicy: Always
  proxyUrl: ""
